#!/bin/bash

set -e  # Parar em qualquer erro

# Cores para output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

echo -e "${BLUE}üöÄ Iniciando ambiente CrewAI + Ollama...${NC}"

# Fun√ß√£o para log com timestamp
log() {
    echo -e "[$(date '+%Y-%m-%d %H:%M:%S')] $1"
}

# Fun√ß√£o para verificar se Ollama est√° rodando
check_ollama() {
    curl -s http://localhost:11434/api/version > /dev/null 2>&1
    return $?
}

# Fun√ß√£o para verificar se Ollama remoto est√° rodando
check_ollama_remote() {
    if [ -n "$OLLAMA_BASE_URL" ]; then
        curl -s "${OLLAMA_BASE_URL}/api/version" > /dev/null 2>&1
        return $?
    fi
    return 1
}

# Fun√ß√£o para verificar se um modelo existe
check_model() {
    local model=$1
    ollama list 2>/dev/null | grep -q "$model"
    return $?
}

# Fun√ß√£o para baixar modelo com retry
download_model() {
    local model=$1
    local max_attempts=3
    local attempt=1
    
    while [ $attempt -le $max_attempts ]; do
        log "${YELLOW}üì• Tentativa $attempt/$max_attempts: Baixando modelo $model...${NC}"
        
        if ollama pull "$model"; then
            log "${GREEN}‚úÖ Modelo $model baixado com sucesso!${NC}"
            return 0
        else
            log "${RED}‚ùå Falha na tentativa $attempt${NC}"
            attempt=$((attempt + 1))
            [ $attempt -le $max_attempts ] && sleep 5
        fi
    done
    
    return 1
}

# 1. Verificar se deve usar Ollama local ou remoto
USE_LOCAL_OLLAMA=false

if [ "$FORCE_MOCK_LLM" = "true" ]; then
    log "${YELLOW}üîß FORCE_MOCK_LLM ativado, pulando configura√ß√£o do Ollama${NC}"
elif check_ollama_remote; then
    log "${GREEN}‚úÖ Ollama remoto dispon√≠vel em $OLLAMA_BASE_URL${NC}"
else
    USE_LOCAL_OLLAMA=true
    log "${BLUE}üîÑ Configurando Ollama local...${NC}"
fi

# 2. Configurar Ollama local se necess√°rio
if [ "$USE_LOCAL_OLLAMA" = "true" ]; then
    if ! check_ollama; then
        log "${BLUE}üîÑ Iniciando Ollama local...${NC}"
        ollama serve > /tmp/ollama.log 2>&1 &
        OLLAMA_PID=$!
        
        # Aguardar Ollama iniciar
        log "${YELLOW}‚è≥ Aguardando Ollama inicializar...${NC}"
        for i in {1..60}; do
            if check_ollama; then
                log "${GREEN}‚úÖ Ollama local iniciado! (PID: $OLLAMA_PID)${NC}"
                break
            fi
            sleep 1
            if [ $i -eq 60 ]; then
                log "${RED}‚ùå Timeout: Ollama n√£o iniciou em 60 segundos${NC}"
                log "${RED}Log do Ollama:${NC}"
                cat /tmp/ollama.log
                exit 1
            fi
        done
    else
        log "${GREEN}‚úÖ Ollama local j√° est√° rodando${NC}"
    fi
fi

# 3. Configurar modelos (s√≥ se Ollama estiver dispon√≠vel)
if [ "$FORCE_MOCK_LLM" != "true" ] && (check_ollama || check_ollama_remote); then
    REQUIRED_MODELS=("${DEFAULT_MODEL:-llama3.2}" "llama3.2:1b")
    
    log "${BLUE}üîç Verificando modelos dispon√≠veis...${NC}"
    ollama list 2>/dev/null || true
    
    MODEL_AVAILABLE=false
    for model in "${REQUIRED_MODELS[@]}"; do
        if check_model "$model"; then
            log "${GREEN}‚úÖ Modelo $model j√° dispon√≠vel${NC}"
            MODEL_AVAILABLE=true
            break
        fi
    done
    
    # Baixar modelo se necess√°rio
    if [ "$MODEL_AVAILABLE" = "false" ]; then
        for model in "${REQUIRED_MODELS[@]}"; do
            if download_model "$model"; then
                MODEL_AVAILABLE=true
                break
            fi
        done
        
        if [ "$MODEL_AVAILABLE" = "false" ]; then
            log "${YELLOW}‚ö†Ô∏è Nenhum modelo dispon√≠vel. Usando Mock LLM como fallback.${NC}"
            export FORCE_MOCK_LLM=true
        fi
    fi
fi

# 4. Verificar estrutura de arquivos
log "${BLUE}üìÅ Verificando estrutura de arquivos...${NC}"

# Criar diret√≥rios necess√°rios
mkdir -p dados logs models temp

# Verificar arquivo de dados principal
if [ ! -f "dados/vales_refeicao.zip" ]; then
    log "${YELLOW}‚ö†Ô∏è dados/vales_refeicao.zip n√£o encontrado${NC}"
    log "${BLUE}üìù Criando estrutura de exemplo...${NC}"
    
    # Criar arquivo README
    cat > dados/README.md << 'EOF'
# Dados para Processamento

## Arquivos Esperados

- `vales_refeicao.zip`: Arquivo principal com planilhas Excel
- Outros arquivos ZIP com dados estruturados

## Formato Esperado

As planilhas devem conter colunas como:
- Data, Valor, Descri√ß√£o, Categoria, etc.

## Como Usar

1. Coloque seus arquivos ZIP na pasta `dados/`
2. Execute o sistema
3. Os resultados aparecer√£o em `logs/` e `temp/`
EOF
fi

# 5. Verificar depend√™ncias Python
log "${BLUE}üêç Verificando depend√™ncias Python...${NC}"
python -c "
try:
    import crewai, pandas, openpyxl, requests
    print('‚úÖ Depend√™ncias principais OK')
except ImportError as e:
    print(f'‚ùå Depend√™ncia faltando: {e}')
    exit(1)
" || {
    log "${YELLOW}üì¶ Instalando depend√™ncias faltantes...${NC}"
    pip install -q crewai pandas openpyxl requests
}

# 6. Status final do sistema
log "${GREEN}üéâ Ambiente configurado com sucesso!${NC}"
echo
log "${BLUE}üîß Status final do sistema:${NC}"

if [ "$FORCE_MOCK_LLM" = "true" ]; then
    echo "   LLM: ü§ñ Mock LLM (modo de desenvolvimento)"
elif check_ollama_remote; then
    echo "   LLM: üåê Ollama remoto ($OLLAMA_BASE_URL)"
    echo "   Modelos: $(curl -s "${OLLAMA_BASE_URL}/api/tags" | grep -o '"name"' | wc -l 2>/dev/null || echo '?') dispon√≠veis"
elif check_ollama; then
    echo "   LLM: üè† Ollama local"
    echo "   Modelos: $(ollama list 2>/dev/null | grep -v "NAME" | wc -l) dispon√≠veis"
else
    echo "   LLM: ‚ùå Nenhum dispon√≠vel"
fi

echo "   Dados: $(ls dados/ 2>/dev/null | wc -l) arquivos"
echo "   Python: ‚úÖ $(python --version)"

# 7. Executar aplica√ß√£o
echo
if [ "$1" = "dev" ]; then
    log "${BLUE}üõ†Ô∏è Executando em modo desenvolvimento...${NC}"
    python main_dev.py
else
    log "${BLUE}üöÄ Executando aplica√ß√£o principal...${NC}"
    python main.py
fi

# Capturar c√≥digo de sa√≠da
EXIT_CODE=$?

if [ $EXIT_CODE -eq 0 ]; then
    log "${GREEN}‚úÖ Aplica√ß√£o finalizada com sucesso${NC}"
else
    log "${RED}‚ùå Aplica√ß√£o finalizada com erro (c√≥digo: $EXIT_CODE)${NC}"
fi

# 8. Manter container ativo para debug se em modo desenvolvimento
if [ "$DEBUG" = "true" ] || [ "$1" = "dev" ]; then
    log "${YELLOW}üîß Modo debug ativo. Container ficar√° rodando...${NC}"
    log "${BLUE}Para acessar: docker exec -it crew_app /bin/bash${NC}"
    
    # Criar um servidor HTTP simples para health check
    python3 -c "
import http.server
import socketserver
import threading

class HealthHandler(http.server.SimpleHTTPRequestHandler):
    def do_GET(self):
        if self.path == '/health':
            self.send_response(200)
            self.send_header('Content-type', 'application/json')
            self.end_headers()
            self.wfile.write(b'{\"status\":\"ok\",\"mode\":\"debug\"}')
        else:
            self.send_response(404)
            self.end_headers()

httpd = socketserver.TCPServer(('', 8000), HealthHandler)
print('Health server rodando na porta 8000...')
httpd.serve_forever()
" &
    
    # Manter container vivo
    tail -f /dev/null
fi

exit $EXIT_CODE